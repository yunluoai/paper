\chapter{测试评估平台的设计与实现}


%*********************************************************************
% 4.1 DSP编译器测试现状分析
%*********************************************************************
\section{DSP编译器测试现状分析}
随着 DSP 编译器后端功能的持续扩展，尤其是全局指令选择框架及相关优化策略的引入，编译器在指令匹配、寄存器组选择以及指令合法化等阶段的行为呈现出更强的输入敏感性和组合多样性。相较于传统的基于DAG的指令选择方案，全局指令选择虽然在实现结构上更加模块化，但其指令选择结果由多个Pass的协同决策共同决定，整体行为对输入程序结构更加敏感。这种全局视角和延迟决策机制显著扩大了可观察的指令选择行为空间，使得潜在缺陷更具隐蔽性，也对测试体系在输入覆盖和结果分析方面提出了更高要求。

\par

DSP工具链虽然已经建立了基本完善的CI（Continuous Integration, 持续集成）测试流程，能够在一定程度上保证主分支代码的功能正确性。然而，该测试体系主要形成于传统编译流程阶段，其设计目标更多集中于功能可用性验证，对于全局指令选择及其相关优化在复杂场景下的正确性和性能稳定性缺乏针对性支持。随着GlobalISel在DSP工具链中的逐步落地，现有测试体系在测试结果可分析性、性能回归检测以及复杂输入场景覆盖等方面逐渐暴露出不足，有必要对其进行系统分析。


% 4.1.1 现有测试流程概述与分析
\subsection{现有测试流程概述与分析}
DSP工具链拥有独立的线上代码托管与协作开发平台。开发者在提交代码后，CI 系统会自动触发一套预定义的测试流程，对最新提交的工具链代码进行编译、链接以及部分运行验证。只有在测试全部通过并经项目负责人审核确认后，相关提交才能被合并至主分支。

\par

该测试流程在当前开发阶段发挥了积极作用。一方面，它能够在代码提交阶段及时发现编译失败、链接错误及明显的功能性缺陷，从流程上保证主分支代码的基本可用性；另一方面，通过统一的CI流水线规范了代码提交流程，减少了人工测试的参与程度，在一定程度上降低了开发成本。

\par

总体来看，尽管现有测试体系在功能正确性验证方面具备基础能力，适用于发现显性错误和阻断明显缺陷进入主分支。然而，该体系主要以是否通过测试为评判标准，对编译器优化效果、性能变化以及复杂场景下的鲁棒性缺乏系统性支撑，难以满足DSP工具链对高性能与高可靠性的长期发展需求。具体体现在测试用例覆盖率不足以及测试产物可读性与分析能力不足这两个问题上。

\par

1.测试集来源单一，场景覆盖局限于常见路径，缺乏随机化测试机制

测试用例的覆盖完整性直接决定了DSP工具链发现潜在缺陷的能力。然而，现有DSP工具链在测试集构成与场景覆盖方面仍存在明显不足，难以对核心编译流程的鲁棒性进行充分验证。从测试来源来看，当前DSP工具链的测试集主要包括以下三类：

\begin{itemize}
	\item
	基于库函数的手写测试用例：该类测试通常围绕基础功能验证设计，重点覆盖 C 标准库、数学库以及运行时支持库等核心库函数的调用场景。
	
	\item
	移植的开源 Benchmark 程序：以通用计算或嵌入式系统性能评估为目标，例如 Embench 等，用于衡量工具链在典型应用负载下的性能表现。
	
	\item 
	已知缺陷的最小复现样例：在开发过程中针对已暴露的问题提炼出的最小测试用例，主要用于回归测试，防止同类缺陷再次引入。
	
\end{itemize}

上述测试用例在验证工具链基础功能正确性以及复现已知缺陷方面具有一定价值，但其整体覆盖范围仍然较为有限。

一方面，测试样例的设计高度依赖人工经验，往往集中于库函数的常规调用路径和标准输入输出场景，对于极端数据类型、异常输入以及复杂控制流等边界条件覆盖不足。这种以“常见路径”为主的测试策略，使得许多非常规但合法的程序形态难以被有效触达。

另一方面，DSP 工具链内部的关键编译阶段缺乏针对复杂场景的系统性测试，尤其是在指令选择、寄存器分配和指令合法化等核心模块中。例如，多寄存器组之间的交叉分配、复杂指令序列的模式匹配，以及非标准位宽数据的合法化转换等场景，往往涉及大量条件组合和路径分支，人工构造测试用例难以穷尽所有可能情况。

\par

这种覆盖不足直接带来两类风险：其一，工具链在通用应用场景下表现正常，但在DSP的特定应用场景中容易暴露出逻辑错误；其二，编译器内部的条件触发型缺陷难以及早发现。这类缺陷通常仅在特定指令组合或数据模式下才会被触发，例如指令选择阶段的模式匹配漏判、寄存器分配中的资源冲突，或合法化转换引入的语义偏差。一旦这些问题未能在测试阶段暴露，往往会在实际应用中引发程序崩溃或性能异常，后果较为严重。

\par

对于引入GlobalISel后的DSP编译流程而言，上述问题进一步被放大。由于指令选择结果往往由多个Pass的协同决策共同决定，例如寄存器组选择与指令合法化之间的相互影响、不同合法化路径对后续指令匹配结果的约束关系等，这类行为通常只会在特定输入结构和数据特征组合下才会显现。依赖人工经验构造测试样例难以覆盖这些复杂交互场景，导致部分潜在缺陷长期隐藏，直至在真实DSP应用中才被触发，从而带来较高的工程风险。

\par

2.测试产物可读性与分析能力不足

现有测试体系在测试结果的表达和分析能力方面同样存在明显不足。当前 CI 流程主要以测试是否通过作为判定标准，测试产物缺乏对编译器后端行为的量化描述，未系统记录程序执行周期、代码尺寸、指令打包效率等关键性能指标，也未对生成的汇编代码进行结构化管理。这使得测试结果只能用于验证功能正确性，而无法反映全局指令选择及相关优化策略对代码性能的实际影响。

\par

此外，测试结果未进行历史化存储和跨版本管理，开发者难以对不同提交之间的性能变化进行对比分析，也无法从整体上观察工具链性能随时间演进的趋势。随着CI测试记录不断积累，大量中间提交和实验性版本产生的结果进一步加剧了测试数据的冗余，显著提高了有效信息筛选和问题定位的成本。

\par

综上所述，现有测试体系在输入覆盖能力和结果分析深度方面均难以支撑全局指令选择框架下编译器后端行为的系统性验证，已无法满足DSP工具链在高性能和高可靠性方向持续演进的需求。


% 4.1.2 测试体系优化需求与技术路径
\subsection{测试体系优化需求与技术路径}
基于前述分析可以看出，随着全局指令选择框架及相关优化策略在DSP工具链中的引入，单纯依赖人工构造测试样例以及是否通过这一单一判定标准，已难以对编译器后端行为进行系统性验证。为保障全局指令选择在复杂应用场景下的功能正确性和性能稳定性，现有测试体系亟需从测试输入覆盖能力和测试结果分析深度两个层面进行针对性优化。

\par

在测试输入层面，测试体系需要具备更强的输入多样性和场景覆盖能力，以支撑全局指令选择框架下条件触发型行为的验证。由于指令选择、寄存器组绑定和合法化等阶段的决策往往依赖于输入程序的控制流结构、数据分布以及运算模式，人工设计测试样例难以系统覆盖所有潜在组合路径。因此，有必要引入随机测试生成机制，通过自动化方式生成大量具有结构多样性和数据随机性的测试程序，从而覆盖人工测试难以触及的边缘场景。这种随机测试方式能够有效暴露隐藏在特定输入条件下的潜在缺陷，提升编译器后端对复杂输入场景的整体鲁棒性。

\par

在测试结果层面，仅验证程序功能是否正确已无法满足全局指令选择优化验证的实际需求。全局指令选择及其相关优化策略的引入，往往不会直接导致功能错误，而是可能以性能退化、代码尺寸膨胀或指令级行为异常等形式体现出来。为准确评估其对DSP架构性能的影响，有必要构建一套能够自动采集、存储和分析性能指标的评估平台，对程序执行周期、代码尺寸及指令级行为进行量化记录，并支持跨版本的对比分析和趋势观察。通过将测试结果结构化存储并以可视化方式呈现，可有效降低性能分析门槛，辅助开发者快速定位性能回退和异常变化。

\par

综上所述，面向全局指令选择框架的DSP工具链测试体系，应当以随机测试驱动的高覆盖输入验证为基础，以性能指标采集与可视化分析为核心支撑，构建一套覆盖广、可量化、可追溯的测试评估机制。基于上述需求，本文在后续章节中设计并实现了一套集随机测试生成与性能评估于一体的测试评估平台，为全局指令选择及其优化策略的验证提供系统化支撑。


%*********************************************************************
% 4.2 系统设计
%*********************************************************************
\section{系统设计}


% 4.2.1 随机测试生成器
\subsection{随机测试生成器}
随机测试生成器是一类用于自动化测试的工具，其核心思想是通过随机或伪随机算法生成大量的测试输入，用于验证软件系统的正确性、稳定性以及鲁棒性。相较于传统依赖人工设计测试用例的方式，随机测试能够在更短时间内覆盖更广泛的输入空间，有利于发现隐藏较深、难以通过经验手段构造的边界错误和组合缺陷。

\par

在编译器领域，随机测试生成器通常指能够自动生成符合编程语言语法和语义约束的源代码程序的测试工具。这类工具通过随机组合表达式、控制流结构、数据类型以及运算符等语言构造要素，来生成大量结构各异的测试程序，并将其作为输入交由编译器处理，从而验证编译器前端解析、优化过程以及后端代码生成等多个阶段的正确性与健壮性。

\par

在编译器随机测试工具的发展过程中，CSmith是较早被广泛应用的一类C语言随机程序生成器，核心目标是发现编译器在处理常规C程序时可能存在的逻辑错误。其特点是生成的随机程序严格遵循C99标准且严格规避未定义行为（Undefined Behavior, UB），从而确保测试结果可复现。CSmith在验证编译器基本语义一致性和防止功能性错误方面发挥了重要作用，但其测试目标主要聚焦于功能是否正确，对编译器在高性能优化场景下的行为覆盖能力相对有限。

\par

随着现代处理器架构复杂度的提升以及编译器优化策略的不断增强，仅依赖功能正确性验证已难以满足编译器工具链的测试需求。在这一背景下，YARPGen（Yet Another Random Program Generator）被提出，其设计初衷正是针对CSmith在性能相关测试方面覆盖不足的问题，重点面向编译器后端的高性能优化阶段开展随机化测试。

\par

与以语义正确性为核心目标的随机测试生成器不同，YARPGen则更加关注编译器在高性能优化场景下的行为表现，重点发现那些在功能层面正确、但在性能或架构适配方面存在问题的深层次缺陷。它可以在不依靠动态检查的情况下，使用生成规则来生成没有未定义行为的表达式，同时提升了生成代码的多样性，触发更多编译器优化的可能。YARPGen在代码生成策略上倾向于构造密集型循环、数组连续访问、数据并行计算以及向量运算等代码模式，从而有针对性地覆盖循环向量化、指令级并行、缓存优化以及指令调度等编译器后端的关键优化阶段。

\par

在语言和架构支持方面，YARPGen不仅支持C语言，还扩展支持C++的部分核心特性，并能够生成面向SIMD、向量指令等现代处理器架构扩展的代码。这一特性使其在测试面向高性能计算、嵌入式DSP以及多媒体处理场景的编译器工具链时具有一定的实用价值。

\par

基于上述特点，本文在测试体系设计中选择基于YARPGen并结合DSP架构特性进行定制化扩展，构建面向全局指令选择及其相关优化策略的随机测试输入生成机制。通过生成具有高性能特征的随机测试程序，测试体系能够更有效地验证编译器在指令选择、寄存器组绑定以及指令合法化等阶段的行为表现，为后续性能评估与回归分析提供可靠的测试基础。


% 4.2.2 系统整体设计
\subsection{系统整体设计}
为解决4.1.1节提到的测试覆盖不足与结果分析能力弱等问题，本文设计并实现一套面向DSP编译器的性能测试评估平台。平台整体由测试子系统与评估子系统两部分构成，前者在原有CI测试平台上进行新功能的添加与旧架构的重构，后者则为全新搭建的独立模块。

\par

测试子系统的核心改进在于移植并定制随机测试生成器YARPGen，为其添加针对DSP指令集架构的内建函数Intrinsic及必要的环境适配逻辑，使其能够在每次测试触发时自动生成一批随机样例，并与原有固定测试集并行执行。

\par

评估部分与线上CI测试平台连接，通过脚本实现编译流程的自动化触发：当编译器代码合并至仓库时。编写脚本使得每次编译器的代码合并到仓库中都会自动在线上模拟器平台运行，同时在连接到远程服务器上的长期工作的芯片运行；测试完成后将结果返回到Gitlab的Perf仓库（便于管理以及迁移）；后端定时拉取Perf仓库并更新数据库，当前端访问时返回查询的结果，并在前端进行展示，可以直观地反映新优化带来的性能提升或退步。

\par

系统整体设计如图\ref{fig:system_flowchart}所示，整体架构分为测试子系统和评估子系统两部分。测试子系统完成从测试样例生成、编译到执行的全自动化流程；评估子系统则通过前端、后端与数据库的协作，实现测试结果的结构化存储、查询、对比分析与可视化展示。两个子系统在功能上相互解耦，通过Perf仓库进行数据交互。该设计既保证了测试流程的高效运行，又提升了测试结果的可分析性与可追溯性。

\begin{figure}[htbp]
	\centering
	\includegraphics[width=1\textwidth]{pics/system_flowchart.png}
	\caption{系统整体设计图}
	\label{fig:system_flowchart}
\end{figure}


% 4.2.3 测试子系统的设计
\subsection{测试子系统的设计}
测试子系统的核心目标是构建全自动的流水线，覆盖从测试用例到结果产出的全流程，具体模块与流程如下：

\begin{itemize}
	\item
	随机测试样例生成模块
	随机测试工具用于自动生成适配DSP架构的随机测试样例，重点覆盖密集循环、向量运算等性能相关场景。随机测试工具在执行测试时再生成，并将错误的样例推送至测试仓库，与其他测试样例一起被统一管理。二者共同为CI测试提供稳定的用例来源。
	
	\item
	编译与测试模块
	CI测试平台作为调度核心，从编译器代码仓库中拉取开发者提交的最新DSP工具链代码，并从测试仓库中获取待执行的测试脚本、测试样例以及需要的测试工具。首先进行编译，编译成功后将测试任务分发至Gitlab Runner，触发两类测试流程：一类是在DSP架构模拟器上执行的测试，用于快速验证功能正确性及基础性能表现；另一类是在实际DSP硬件芯片上执行的测试，用于发现硬件上的Bug，以及获取更加准确的性能指标。
	
	\item
	结果处理模块
	测试完成后，CI测试平台会将所有测试结果统一归档，并推送至Perf仓库。归档内容不仅包括测试通过或失败状态，还涵盖性能指标数据以及生成的汇编代码，为后续评估分析提供完整、可靠的数据基础。除此之外，为了解决4.1.2中提到的冗余问题，设置过滤器来保证只有主分支的代码才会传到Perf仓库中。
	
\end{itemize}


% 4.2.4 评估子系统的设计
\subsection{评估子系统的设计}
评估部分以测试结果高效利用与直观呈现为目标，采用前端、后端和数据库协同工作的三层架构，对测试数据进行统一管理和深度分析。具体模块与流程如下：

\begin{itemize}
	\item
	后端模块
	后端模块负责从Perf仓库拉取性能报告，对原始数据进行结构化解析并将数据存储至数据库；同时，后端对外提供统一的数据接口，支持测试结果查询、跨版本对比以及性能趋势分析等功能，为前端的可视化展示提供服务支撑。
	
	\item
	数据库模块
	数据库模块作为测试结果的持久化存储中心，统一保存所有历史测试数据，支持按照芯片型号、Commit号、优化等级、测试集以及测试样例等多维条件进行快速检索。这一设计为性能回归分析、版本对比以及长期趋势观察提供了坚实的数据基础。
	
	\item
	前端模块
	前端模块则为用户提供直观、交互友好的可视化界面。用户可以通过前端查看单个提交版本的测试结果，对比不同提交之间的性能差异，或以时间序列方式观察性能变化趋势等。同时，前端支持灵活的数据筛选操作，例如仅查看片上测试的性能指标，相关请求由后端实时处理并从数据库中返回结果进行展示。
	
\end{itemize}


%*********************************************************************
% 4.3 系统实现
%*********************************************************************
\section{系统实现}


% 4.3.1 基于DSP定制的YARPGen实现
\subsection{基于DSP定制的YARPGen实现}
YARPGen是一个通用的编译器测试用例生成工具，其原生设计主要针对通用CPU架构及标准C/C++运行环境。在默认实现中，YARPGen假设目标平台具备标准的运行时库支持，并依赖stdio等通用接口完成程序结果的输出与校验。然而，DSP后端在硬件特性、调试机制以及运行环境方面均与通用处理器存在显著差异，这使得原生YARPGen难以直接应用于DSP编译器流水线的自动化验证。

\par

为使随机测试生成机制能够适配到DSP编译器后端，本文对YARPGen进行了针对DSP架构的定制化扩展。整体改动遵循可复用和可扩展的设计原则，在保留YARPGen原有随机生成能力的基础上，引入对DSP平台特性的适配支持。具体改动主要体现在以下几个方面。

\par

1.引入DSP专用调试头文件

DSP编译器的测试与验证依赖架构专用的调试与输出接口，而非标准的stdio输出机制。为保证生成的随机测试程序能够在DSP环境中正确输出运行结果，需在测试用例中引入特定头文件以支持调试输出和结果对比。在YARPGen的emitCheckFunc阶段，通过条件编译区分通用平台和DSP平台，为DSP环境添加专用调试头文件，并声明调试输出相关的函数接口。相关代码片段如下所示：

\lstset{language=c++}
\begin{lstlisting}
void emitCheckFunc(std::ostream &stream) {
	std::ostream &out_file = stream;
	out_file << "#ifdef DSP_VALIDATION\n";
	out_file << "#include <swift_debug.h>\n";
	out_file << "#else\n";
	out_file << "#include <stdio.h>\n\n";
	out_file << "#endif\n";
	...... }
\end{lstlisting}

其中，DSP\_VALIDATION用于标识当前测试是否运行于DSP验证环境。该宏由测试流水线在编译阶段统一注入，从而保证同一套测试生成逻辑可同时服务于通用平台与DSP平台，避免代码分叉维护。通过该方式，YARPGen在DSP模式下能够调用调试头文件中提供的调试输出接口，实现与DSP仿真器或硬件调试环境的对接。

2.添加DSP专用调试输出逻辑

在通用平台中，YARPGen通过标准输出对程序执行结果进行校验；而在DSP平台上，测试结果需要通过专用调试通道输出，以便测试框架进行采集与比对。为此，在YARPGen的emitMain阶段中，通过条件编译为DSP环境下添加调试函数调用，用于输出校验结果。

3.引入目标架构选项与条件编译参数

原生YARPGen只区分生成语言，并不区分目标后端架构。为支持DSP平台相关逻辑，本文在YARPGen中引入了目标架构选项，并通过条件编译参数控制测试生成与代码发射阶段的具体行为。这一改动为后续引入DSP架构的Intrinsic提供了基础设施支持。

4.添加DSP架构的Intrinsic

为了更有效地测试DSP编译器在指令选择与优化阶段的行为，本研究在YARPGen中引入了DSP架构专用Intrinsic，并将其纳入随机测试生成范围。由于YARPGen不支持Intrinsic，于是需要拓展其功能，在IRNode中引入新的Intrinsic选项，并根据操作数数量进行分类，如图\ref{fig:irnode_struct}所示。以SIN函数为例，对YARPGen的IRNode生成逻辑进行了扩展。

\begin{itemize}
	\item
	createHelper：核心作用是创建SIN函数调用表达式，包括生成算术表达式参数、处理布尔类型转换，最终返回封装后的IntrinsicExpr指针。这是YARPGen中表达式生成逻辑的关键部分，用于随机生成SIN调用的测试用例。
	
	\item
	propagateType：核心作用是对SIN函数的两个参数表达式进行类型推导、类型提升和类型统一，最终确定SIN函数调用表达式的返回值类型。这是代码生成过程中保证类型安全和语法正确性的关键步骤。
	
	\item 
	evaluate：在测试生成或运行时阶段计算SIN函数调用的结果，包括处理参数的类型转换、未定义行为判断，以及最终返回计算后的标量值。这是YARPGen中表达式求值逻辑的关键部分，用于确定SIN函数调用的实际结果值。
	
	\item
	emitCDefinitionImpl：核心作用是为C语言环境生成SIN函数的宏定义实现。因为C语言标准库中没有通用的SIN函数，该方法通过输出C语言的语句表达式宏，为SIN提供兼容C语言的实现，保证生成的测试用例在C环境下能正常编译运行。
	通过上述的定制化扩展，YARPGen被成功适配到DSP编译器的流水线测试场景中。基于条件编译的设计使得：同一套随机测试生成逻辑可同时服务于通用平台与DSP平台；DSP专用Intrinsic能够被系统性纳入随机测试覆盖范围。
	
\end{itemize}

\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.8\textwidth]{pics/irnode_struct.png}
	\caption{拓展后的IRNode结构图}
	\label{fig:irnode_struct}
\end{figure}

对应的类层次结构如图\ref{fig:yarpgen_class}所示。

\begin{figure}[htbp]
	\centering
	\includegraphics[width=1\textwidth]{pics/yarpgen_class.png}
	\caption{扩展后的类结构}
	\label{fig:yarpgen_class}
\end{figure}

% 4.3.2 测试子系统的实现
\subsection{测试子系统的实现}
测试部分是性能测试评估平台的核心组成之一，其主要任务是在原有功能正确性验证的基础上，引入对编译器性能表现的自动化测量能力，并将测试流程整体融入持续集成流水线中，实现性能测试的标准化与可重复执行。围绕这一目标，测试部分的实现主要包括性能指标测量支持的引入以及测试脚本逻辑的重构两个方面。

\par

（1）添加测量性能指标的支持
衡量VLIW架构的DSP编译器性能的核心指标包括程序运行周期数、生成代码的大小、汇编代码质量以及指令的打包效率等。上述指标分别从执行效率、存储资源占用以及指令层级优化效果等角度反映了编译器后端的整体性能表现。
针对不同性能指标的获取方式，测试系统采用了差异化的实现策略。代码大小的测量通过集成开源工具完成，工具通过在编译完成后对目标文件进行分析，来自动统计生成代码所占用的存储空间。汇编代码的获取则依赖于DSP工具链自带的反汇编器，在编译阶段结束后对目标文件进行反汇编，生成对应的汇编输出，为后续的指令级分析提供基础数据。指令打包效率由工具链的模拟器进行统计，该模拟器能够在模拟执行过程中收集指令发射和打包相关信息，从而反映编译器在指令调度与并行化方面的优化效果。

\par

运行周期数的获取在实现上具有一定复杂性。对于模拟器环境，程序运行周期数可以在执行过程中直接由模拟器提供；然而在实际DSP芯片上，周期数的测量需要依赖底层驱动库所提供的硬件计数器接口。为使周期数测量能够无缝融入自动化测试流水线，平台对原有驱动库进行了精简与重构，将其裁剪为可重定向的独立文件，仅保留初始化和时间戳获取两个核心函数。在链接阶段，这些函数被统一链接至测试程序的主函数中，从而实现对程序执行前后时间戳的自动采集。通过这一方式，芯片上的运行周期数测量能够与编译、执行流程紧密结合，实现流水线化和自动化。

\par

（2）重构测试脚本并添加YARPGen
原先的测试脚本的逻辑是只判断样例执行正确与否，无法满足性能评估平台对多维性能数据采集的需求，为此，需要对测试脚本的整体逻辑进行重写和扩展。在新的脚本中，测试流程在完成编译与执行后，会自动触发性能数据采集模块，分别获取运行周期数、代码大小、汇编输出以及指令打包效率等指标，并将其统一整理为结构化的性能报告。

\par

性能报告以标准化格式生成，便于后续自动解析和存储。在报告生成完成后，测试脚本会将测试结果进行打包，并自动上传至Perf仓库，作为后续评估和可视化分析的数据源。此外，为提升测试覆盖度，CI脚本中还集成了YARPGen的随机样例生成逻辑，在每次测试触发时可自动生成一批新的随机测试程序，并与已有测试样例共同参与测试执行。通过这一机制，测试流水线不仅能够验证固定测试集上的功能和性能表现，还能够持续引入新的输入场景，提升对潜在缺陷的发现能力。


% 4.3.3 评估子系统的实现
\subsection{评估子系统的实现}

1.数据展示层的实现

评估部分用于对测试阶段产生的大量性能数据进行集中管理、分析与展示，其实现采用典型的三层架构，包括前端展示层、后端服务层以及数据库存储层。各层在功能上相互解耦，在数据流转上通过标准化接口进行协作，从而保证系统的可扩展性与维护性。
前端部分基于Vue框架实现交互界面，并结合Apache ECharts实现性能数据的图形化展示。通过前端界面，用户可以直观地查看单个提交版本的测试结果，对比不同提交之间的性能差异，并以时间序列形式观察性能指标的变化趋势。可视化展示方式有效降低了性能分析门槛，使开发者能够快速定位性能回退或异常波动问题。在实现过程中，前端通过组件化方式对不同功能模块进行拆分，具体模块如下：
（1）Dashboard
用于展示与竞品芯片性能指标的直观对比
（2）Summary：
用于查看历史Commit的宏观信息，针对测试集整体趋势
（3）History：
用于查看历史Commit的宏观信息，针对单独测试样例的趋势
（4）Compare：
用于对比两个Commit的性能指标
（5）Custom：
用于上传本地Perf来与主分支测试集进行比较

\par

前端通过调用后端提供的REST接口获取性能数据，并在本地对数据进行初步处理和筛选，以减少不必要的数据传输。性能数据的图形化展示采用Apache ECharts实现，针对不同分析场景选用合适的图表类型：例如，使用柱状图展示与竞品芯片在相同测试集下执行周期数或代码大小上的对比结果，使用折线图展示执行周期数及代码大小随提交版本变化的趋势情况。通过这种方式，前端能够直观反映性能变化特征，帮助开发者快速识别性能回退或异常波动。

2.业务逻辑层的实现

后端部分基于Spring Boot框架实现，负责性能数据解析、业务逻辑处理以及接口服务等任务。后端在启动或触发定时任务时，会从Perf仓库中拉取测试结果，并按照预定义的数据格式对原始JSON数据进行解析。解析过程中，后端会将测试环境信息（如目标处理器、优化等级、测试平台）与单个测试样例的性能指标进行关联建模，从而形成结构化的数据对象。随后，这些数据通过MyBatis持久层框架映射到数据库表结构中，实现性能数据的持久化存储。为保证在多用户查询或批量数据写入场景下系统的稳定性，后端引入Druid数据库连接池，对数据库连接进行统一管理与复用，有效降低了连接创建和释放的开销。

\par

在接口设计方面，后端向前端提供了一组面向性能分析场景的查询接口，例如按提交版本查询单次测试结果、对比两个或多个提交之间的性能差异，以及获取某一性能指标的历史变化数据等。后端在接口层完成对性能数据的筛选、聚合和排序操作，使前端无需关心底层数据组织方式即可完成复杂查询，提升了系统整体的模块化程度。

\par

3.数据存储层的实现
数据库层采用MySQL实现，用于持久化存储测试结果。在数据库设计过程中，结合性能分析的典型使用场景，对数据表进行了规范化建模，将测试任务信息、测试样例信息以及性能指标信息分别存储，并通过外键关系进行关联。数据库支持按照提交版本、测试类型和性能指标等维度进行高效查询，为跨版本性能对比和长期趋势分析提供了数据基础。随着测试数据的持续积累，数据库能够形成完整的性能历史记录，为DSP工具链优化效果的量化评估和回归分析提供可靠支撑。







